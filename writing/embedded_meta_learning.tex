\documentclass[11pt]{article}

\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{relsize}
\usepackage{natbib}
\usepackage{float}
\usepackage[margin=1in]{geometry}

\floatstyle{boxed}
\restylefloat{figure}

\begin{document}


\section{Introduction}
Humans are able to use and reuse knowledge more flexibly than most deep learning models can \citep{Lake2017}. One fundamental reason for this is the siloing of knowledge within deep neural networks -- although deep networks represent knowledge about data (activations) and knowledge about transformations of that data (parameters), they do not represent any relationships between these. By contrast, humans are (or can be) consciously aware of both our representations of input, the transformations we can perform on them, and the relationships between data and functions on the data. This is true for domains from game playing to natural language understanding. \par
One of the abilities that this grants humans is the ability to learn rapidly from relatively few examples. This problem has been partially addressed by meta-learning systems \citep{}. Yet humans can then take this knowledge and flexibly alter their behavior in accordance with a change in task demands or a single instruction. Deep learning systems at present lack this flexiblity. \par
In this paper, we propose a new class of architectures which essentially take a \emph{functional programming perspective} on meta-learning. By treating both data and transformations on the data as functions, we can conceptually think of both as transformable data. This yields the ability to not just learn to solve new tasks, but to learn how to flexibly transform these solutions in response to changing task demands. We suggest that this ability will be key to building more intelligent deep learning systems. \par

\section{Method}


\section{Results}

\section{Discussion}

\end{document}
