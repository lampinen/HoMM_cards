\documentclass{article}

\usepackage{neurips_2019_author_response}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{lipsum}

\begin{document}
We thank the reviewers for their thoughtful and thorough reviews. We have attemtped to respond below. \par
\textbf{miniImageNet:} \par
\looseness=-1
\textbf{Novelty and contributions:} As all three reviewers noted, the most novel contribution of our paper is the proposal to thing about transfer in terms of meta-mappings and task transformations. The architecture we propose is designed with this idea in mind. It is true that previous work has considered task embeddings (some of the works suggested by the reviewers we have mentioned in the related works section, but some we were not familiar with, we appreciate the pointers). Most of these have developed some way of embedding a dataset into some latent space. However, what we think is fundamentally novel about our architecture is that \textbf{we use the same latent space for the dataset as we use for an individual input to the network}. This allows us to use the same meta and hyper networks for both basic tasks and meta-mappings, which we show in appendix D.1 performs better than separate embedding spaces for basic inputs and tasks. We think that this is a fundamentally novel aspect of our architecture, since it is primarily useful for the types of tasks we propose. While other techniques that create explicit task embeddings could also be trained to transform them for meta-mappings, we think our novel insight of a shared embedding space for individual data points and tasks would result in better performance in these domains too. We do think that combining other meta-learning approaches such as MAML with a system like ours is an interesting direction for future work. As we point out in the future directions, we do not expect the exact approach we've proposed to be the best way of solving these problems. However, it does provide a useful starting point. We think that the primary contribution of this work is highlighting the idea of meta-mappings for transfer, and providing some starting points for how to approach them by taking a functional perspective, which we hope will inspire future work. \par
\looseness=-1
\textbf{Meta-mapping training details:} Consider a meta-mapping task, like trying to map from winning games to losing them. For concreteness, imagine we have three tasks, chess, poker, and go, and losing variations of each. We generate task embeddings for each game, e.g. $z_{poker}$, by applying the meta network $\mathcal{M}$ to experiences from the game. We randomly select some games (poker, go) to use as training examples to generate the meta-mapping parameters, and hold-out the rest (chess) to make the generated meta-mapping generalize. We thus generate a dataset $D = {(z_{poker}, z_{lose-poker}), (z_{go}, z_{lose-go})}$. Per above, we process this dataset using the same meta-network $\mathcal{M}$ to get a function embedding $z_{lose} = \mathcal{M}(D)$ for the meta-mapping. We can then pass this embedding through the hypernetwork $\mathcal{H}$ in order to parameterize a mapping $F_{z_lose}$ which attempts to map game strategies to strategies for the losing variation of that game. We use this mapping to transform each input embedding and compute the $\ell_2$ loss between the result and the target function embedding, e.g. for chess the loss would be $(F_{z_lose}(z_{chess}) - z_{lose-chess})^2$. We minimize this loss across all held-out and training examples (using both accelerates learning). We continue with the next meta-mapping. \par
\looseness=-1
To evaluate performance on a meta-mapping, we generate the training dataset as above, but we do not assume we have an embedding for the held-out targets. For example, we may know how to play chess via a task embedding $z_{chess}$, but our goal is to figure how to lose at chess. So we take $\hat{z}_{lose-chess} = F_{z_{lose}}(z_{chess})$, and use it to play losing chess by parameterizing a function $F_{\hat{z}_{lose-chess}}$. We then evaluate this function's actual performance on losing chess, which tells us the true quality of the meta-mapping. We hope this clarifies the training and evaluation of meta-mappings. We have revised the manuscript to be more explicit about this and fix typos noted by reviewer 1, and if our manuscript is accepted we will also incorporate more details of tasks and training from the appendix as space allows. \par
\looseness=-1
\textbf{Datasets:} We will release the code for this paper on github and provide links in the de-anonymized version, and both the polynomials and cards tasks and baselines are implemented as stand-alone python modules that other researchers can use. As we mention in the future directions, creating complex datasets for meta-mappings is an important future direction. \par
\looseness=-1
\textbf{Architecture:} The reviewers noted that some of the architectural choices seemed arbitrary. We explored some variations in appendix D to justify a few hoices, and showed that some more parsimonious choices resulted in worse performance. However, we acknowledge that we have not explored the full space of models or algorithms which could be used to approach this problem, and we see this as an important direction for future work. This problem is not exactly a standard supervised problem because you predict rewards for several actions, but only observe the reward for the action you execute, but we are not using a full DQN. We have revised the manuscript to clarify this point. \par 
\looseness=-1
\textbf{Natural language:} was simply used as a meta-mapping cue in the main text. Specifically, we used the language embedding as input to the hyper network to generate the meta-mapping parameters. In appendix A we compare to specifying new tasks directly from language. Meta-mappings perform better, presumably by leveraging task knowledge. \par
\looseness=-1
\textbf{Continual learning:} Any architecture which generates task embeddings could cache them, but we have not seen prior work use this. We think the memory requirements of this approach compare favorably to many other approaches for continual learning, since it only requires storing an embedding vector per task, whereas many approaches (such as EWC) require storing an extra ``importance'' parameter for each parameter in the model. For modern models with up to billions of parameters, caching a relatively low-dimensional embedding per task would consume even up to hundreds of thousands of tasks. We think this observation is somewhat interesting, but it is not the main point of our article, so we have slightly de-emphasized it. \par

\end{document}
