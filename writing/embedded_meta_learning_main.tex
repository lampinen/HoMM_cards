\documentclass{article}

\usepackage{neurips_2019}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[pdfborder={0 0 0}]{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{relsize}
\usepackage{natbib}
\usepackage{float}
\usepackage{tikz}

\usetikzlibrary{shapes,arrows}

\tikzstyle{block} = [rectangle, draw, thick, align=center, rounded corners]
\tikzstyle{boundingbox} = [very thick, dotted, gray]
\tikzstyle{dashblock} = [rectangle, draw, thick, align=center, dashed]
\tikzstyle{conc} = [ellipse, draw, thick, dashed, align=center]
\tikzstyle{netnode} = [circle, draw, very thick, inner sep=0pt, minimum size=0.5cm]
\tikzstyle{relunode} = [rectangle, draw, very thick, inner sep=0pt, minimum size=0.5cm]
\tikzstyle{line} = [draw, very thick, -latex']

\setlength{\parskip}{1em}
\setlength{\parindent}{0em}

\setcitestyle{square}

\begin{document}
\title{Embedded Meta-Learning: Toward more flexible deep-learning models}
\author{%
Andrew K. Lampinen\\
Department of Psychology\\
Stanford University\\
\textt{lampinen@stanford.edu}\\
\And
James L. McClelland\\
Department of Psychology\\
Stanford University\\
\textt{mcclelland@stanford.edu}\\
}
\date{}
\maketitle

\begin{abstract}
How can deep learning systems flexibly reuse their knowledge? Toward this goal, we propose a new class of challenges, and a class of architectures that can solve them. The challenges are meta-mappings, which involve systematically transforming task behaviors to adapt to new tasks zero-shot. We suggest that the key to achieving these challenges is representing the task being performed along with the computations used to perform it. We therefore draw inspiration from meta-learning and functional programming to propose a class of Embedded Meta-Learning (EML) architectures that represent both data and tasks in a shared latent space. EML architectures are applicable to any type of machine learning task, including supervised learning and reinforcement learning. We demonstrate the flexibility of these architectures by showing that they can perform meta-mappings, i.e. that they can exhibit zero-shot remapping of behavior to adapt to new tasks. 
\end{abstract}

\section{Introduction}
Humans are able to use and reuse knowledge more flexibly than most deep learning models can \citep[e.g.][]{Lake2016, Marcus2018}. One fundamental reason for this is that humans are aware of what we are trying to compute and why. By contrast, there is a fundamental separation of knowledge within most deep neural networks -- although deep networks represent knowledge about data (in their activations) and knowledge about the structure of tasks (in their parameters), they do not represent any relationships between data and tasks. That is, a neural network's knowledge about what is being computed is only implicitly accessible to those computations. \par
There are a number of advantages to representing knowledge about data and tasks together. In particular, it can grant the ability to rapidly adapt behavior to a new task. The problem of rapid learning has been partially addressed by meta-learning systems \citep[e.g.][]{Vinyals2016, Finn2017a}. However, humans can use our knowledge of a task to flexibly alter our behavior in accordance with a change in task demands or a single instruction. For example, once we learn to play a game, we can immediately switch to playing in order to lose, and can achieve reasonable performance without any retraining (i.e. zero-shot). Deep learning systems at present generally lack this representational flexibility. \par
In this paper, we propose a new class of tasks based on this idea: meta-mappings, i.e. mappings between tasks (see below). This type of transfer is easily accessible to humans \citep{Lake2016}, but is generally inaccessible to most deep-learning models. To address this challenge, we propose a new class of architectures which essentially take a functional perspective on meta-learning, and exploit the idea of homoiconicity. (A homoiconic programming language is one in which programs in the language can be manipulated by programs in the language, just as data can.) By treating both data and task behaviors as functions, we can conceptually think of both data \emph{and} learned task behaviors as transformable. This yields the ability to not only learn to solve new tasks, but to learn how to transform these solutions in response to changing demands. We demonstrate that our architectures can flexibly remap their behavior to address the meta-mapping challenge. EML may also offer other benefits, such as a useful framework for continual learning. We suggest that approaches like EML will be key to building more intelligent and flexible deep learning systems. \par

\section{Meta-mappings}
We propose the challenge of meta-mappings, that is, tasks which take tasks as an input or output. These include mapping from tasks to language (explaining), mapping from language to tasks (following instructions), and mapping from tasks to tasks (adapting behavior). While the first two categories have been partially addressed in prior work \citep[e.g.][]{Hermann2017, Co-Reyes2019}, the latter is more novel. \par
We argue that these task-to-task meta-mappings are a useful way to think about human-like flexibility, because a great deal of our rapid adaptation is from a task to a variation on that task. For example, go on a large board and go on a small board share a great deal of structure. Humans can exploit this to immediately play well on a different board, but deep learning models generally have no way of achieving this. We can also adapt in much deeper ways, for example fundamentally altering our value function on a task, such as trying to lose. While meta-learning systems can rapidly learn a new task from a distribution of tasks they have experience with, this does not fully capture human flexibility. We can adapt to substantial task alterations zero-shot, that is, without seeing a single example from the new task. We suggest that meta-mappings offer a way to understand this flexibility, and a way for deep-learning models to achieve it.\par
Their are different ways this adaptation can be cued, for example by seeing examples of similar task mappings (``you've played checkers on a regular and half-size board, now try to adapt your go strategy to a smaller board,'') or simply by natural-language instructions (``play go on this smaller board'', see appendix \ref{app_why_meta_mapping} for a comparison to conditioning on a task description.). Achieving the flexibility to adapt to meta-map to new tasks will be an important step towards more general intelligence -- intelligence that is not limited to exactly the training domains it has seen. \par 

\section{Embedded Meta-Learning (EML) architecture}
To address these challenges, we propose EML architectures, which are composed of two components: 
\vspace{-1em}
\begin{enumerate} \setlength \itemsep{0em}
\item An input/output system: domain specific encoders and decoders (vision, language, etc.) that map into a shared embedding space $Z$.
\item A meta-learning system which embeds tasks into the shared embedding space $Z$, and learns to map task embeddings to behavior.
\end{enumerate}
\vspace{-0.75em}
Why have a completely shared space $Z$ for data and tasks? The primary advantage is that it allows for arbitrary mappings between these entities. In addition to basic tasks, the system could in principle learn to map language to tasks (follow instructions) or tasks to language (explain behavior), or meta-map tasks to tasks (change behavior). This is a step closer to the human flexibility. \par
Without training on meta-mappings, of course, the system will not be able to execute them well. However, if it is trained on a broad enough set of such mappings, it will be able to generalize to new instances drawn from the same data/task distribution, as we demonstrate below. For instances that fall outside its data distribution, or for optimal performance, it may require some retraining, however. This reflects the structure of human behavior -- we are able to learn rapidly when new knowledge is relatively consistent with our previous knowledge, but learning an entirely new paradigm (such as calculus for a new student) can be quite slow \citep[cf.][]{Kumaran2016}. \par 
\begin{figure}
\centering
\begin{tikzpicture}[auto, scale=0.75]
\node [dashblock] at (0, 0) (rep) {\begin{tabular}{c}$\bm{Z}$: \textbf{Shared} \\ \textbf{representation}\end{tabular}};

% inputs, outputs

\draw [boundingbox] (-9, -5) rectangle (-2.1, 5); 
\node [text=gray] at (-8.5, 4.65) {\textbf{I/O}};

\node [conc] at (-7, -4) (perc) {\textbf{Input}};
%%\node [conc] at (-7.25, -2.5) (lang) {\textbf{Language}};
\node [conc] at (-7, 4) (act) {\textbf{Output}};
\node [block, text width=2cm] at (-4.5, -2) (IE) {$\bm{\mathcal{I}}$: \textbf{Input encoder}};
%%\node [block, text width=2cm] at (-5, -0.5) (LE) {$\bm{\mathcal{L}}$: \textbf{Lang. Encoder}};
\node [block, text width=2.2cm] at (-4, 2.5) (OD) {$\bm{\mathcal{O}}$: \textbf{Output decoder}};
\node [block, text width=2cm] at (-5.5, 1) (TE) {$\bm{\mathcal{T}}$: \textbf{Target encoder}};
\path [line] (perc.north) to (IE.south);
%%\path [line] (lang.north) to ([xshift=0.05cm, yshift=0.05cm]LE.south west);
\path [line] ([xshift=-0.05cm, yshift=-0.05cm]IE.north east) to (rep.south west);
%%\path [line] (LE.east) to (rep.west);
\path [line] (rep.north west) to ([xshift=-0.05cm, yshift=0.05cm]OD.south east);
\path [line] (OD.north) to (act.south);
\path [line] (act.south) to (TE.north);
\path [line] (TE.east) to (rep.north west);

% meta
\draw [boundingbox] (-1.9, 5) rectangle (6, -5); 
\node [text=gray] at (-0.4, 4.7) {\textbf{Meta Learner}};

\node [dashblock] at (0, -2.5) (collection) {
\(\left\{
\begin{matrix}
(z_{in,0}, z_{out,0}),\\
(z_{in,1}, z_{out,1}),\\
$\vdots$
\end{matrix}\right\}\)};
\path [line] (rep.south) to (collection);
\path [line] ([xshift=-1em]rep.south) to (collection);
\path [line] ([xshift=1em]rep.south) to (collection);

\node [block] at (4, 0) (meta) {\begin{tabular}{c}$\bm{\mathcal{M}}$: \textbf{Meta} \\ \textbf{network}\end{tabular}};
\path [line] (collection.south) to [out=-90, in=-90] (meta.south);
\path [line] (meta.west) to (rep.east);

% hyper

\node [block] at (4, 3) (hyper) {\begin{tabular}{c}$\bm{\mathcal{H}}$: \textbf{Hyper} \\ \textbf{network}\end{tabular}};
\node [block, dash pattern=on 9pt off 2pt] at (0, 2) (transform) {\(F: \text{rep} \rightarrow \text{rep}\)};

\path [draw, ->, very thick] (rep.north east) to (hyper.south);
\path [draw, ->, very thick] (hyper.west) to (transform.north east);
\path [draw, ->, very thick] ([xshift=-1em]transform.south) to ([xshift=-1em]rep.north);
\path [draw, ->, very thick] ([xshift=1em]rep.north) to ([xshift=1em]transform.south);

\end{tikzpicture}
\caption{The EML architecture. Blocks with solid edges denote learnable deep networks, dashed edges represent inputs, outputs, embeddings, etc., and $F$ is a deep network with parameters specified by $\mathcal{H}$. Note that there may be multiple input, output, and target decoders if the system has multiple input modalities, e.g. both language and vision. See appendix \ref{app_model_details} for details.} \label{architecture_fig}
\end{figure}
More formally, we take a \emph{functional} perspective on learning. A datum can be represented by a constant function which outputs it. (For example, each point in the latent space of an autoencoder can be thought of this way.) This allows us to interpret model inputs or outputs as functions. We can then interpret most machine learning tasks as a mapping of functions to functions. The key insight is that, while these mappings could be over functions that represent data\footnote{Where ``data'' is a quite flexible term. The approach is relatively agnostic to whether the learning is supervised or reinforcement learning, whether inputs are images or natural language, etc.}, they could also operate on functions that represent tasks. This is related to the idea of a homoiconicity, defined above. Under this perspective, learning tasks, learning to map from language to tasks, and learning to flexibly transform between tasks, are all the same type of problem. \par
Specifically, we embed inputs, targets, and mappings into a shared representational space $Z$ (see appendix \ref{app_lesion_results_shared_z} for some discussion of why this space should be shared). Inputs are embedded by a deep network $\mathcal{I}: \text{input} \rightarrow Z$. Model outputs are decoded from $Z$ by a deep network $\mathcal{O}: Z \rightarrow \text{output}$. Target outputs are encoded by a deep network $\mathcal{T}: \text{targets} \rightarrow Z$.\par
Given this, the task of mapping inputs to outputs can be framed as trying to find a transformation of the representational space that takes the (embedded) inputs from the training set to embeddings that will decode to the target outputs. These transformations are performed by a system with the following components (see Fig. \ref{architecture_fig} for a schematic): $\mathcal{M}: \{(Z, Z), ...\} \rightarrow Z $ -- the meta network, which takes a set of (input embedding, target embedding) pairs and produces a function embedding. $\mathcal{H}: Z \rightarrow \text{parameters}$ -- the hyper network, which maps a function embedding to parameters. $F: Z \rightarrow Z$ -- the transformation, implemented by a deep network parameterized by $\mathcal{H}$. \par
\vspace{-0.5em}
\paragraph{Basic operation:} To perform a basic task, first, a training dataset is encoded by $\mathcal{I}$ and $\mathcal{T}$. These examples are fed to $\mathcal{M}$ to produce a function embedding. This function embedding is mapped through $\mathcal{H}$ to parameterize $F$, and then $F$ is used to process a second dataset, and $\mathcal{O}$ to map the resultant embeddings to outputs. This system can be trained end-to-end on targets for the second dataset. See appendix \ref{app_model_details} for detailed architecture, operation, and hyper-parameters. \par 
More explicitly, suppose we have some dataset of input, target pairs ($D_1 = \{(x_0, y_0), ...\}$), and some input $x$ from some other dataset $D_2$ for which to predict an output $\hat{y}$. $\hat{y}$ would be generated as follows: 
\[\hat{y} = \mathcal{O}\left(F_{D_1}\left(\mathcal{I} \left(x\right)\right) \right)\]
where $F_{D_1}$ is the transformation the meta-learner guesses for the training dataset $D_1$:
\[F_{D_1} \text{ is parameterized by } \mathcal{H}\left(\mathcal{M}\left( \left\{\left(\mathcal{I}\left(x_0\right), \mathcal{T}\left(y_0\right) \right), \left(\mathcal{I}\left(x_1\right), \mathcal{T}\left(y_1\right) \right), ... \right\}\right)\right)\]
This system can be trained end-to-end if labels are provided for the second set of inputs $D_2$. In particular, suppose we have some loss function $\mathfrak{L}(y, \hat{y})$ defined on a single target output $y$ and actual model output $\hat{y}$, for some input $x$. We define our total loss computed on some dataset $D_2$ as:
\[\mathbb{E}_{(x, y)\in {D}_2} \left[ \mathfrak{L}\left(y, \mathcal{O}\left(F_{D_1}\left(\mathcal{I} \left(x\right)\right) \right)\right)\right]\]
\paragraph{Meta-mapping:} However, the system is not limited to simply mapping inputs to outputs. The key advantage of our architecture is that it allows transforming anything that is embedded in $Z$ using the same system. Because tasks are embedded in $Z$, this allows for meta-mappings. For example, suppose we have an embedding $z_{go} \in Z$ for the task of playing go. We can generate a meta-mapping embedding $z_{\text{meta}} \in Z$ from examples of winning and trying-to-lose various games: $z_{\text{meta}} = \mathcal{M}\left( \left\{\left((z_{checkers},z_{checkers,lose}\right), ... \right\}\right)$. We can generate a new task embedding $\hat{z}_{new} \in Z$:  
\[\hat{z}_{new} = F_{z_{\text{function}}}(z_{go}) \qquad \text{where } F_{z_{\text{meta}}} \text{ is parameterized by } \mathcal{H}\left(z_{\text{meta}}\right)\]
This $\hat{z}_{new}$ could be interpreted as the system's guess at a losing strategy for go. We could then evaluate whether the system loses with the $\hat{z}_{new}$ strategy. All meta-mapping results reported here were evaluated in that way -- evaluating the loss on the target task after transforming the task embedding. \par
Alternatively, we could map from language to a meta-mapping embedding, rather than inducing it from examples of the meta-mapping. This corresponds to the human ability to change behavior in response to instructions. The key feature of our architecture -- the fact that tasks, data, and language are all embedded in a shared space -- allows for substantial flexibility within a unified framework. 

%\subsection{Boolean functions}
%As a proof of concept, we first evaluated the system on a simple task of computing boolean functions on boolean inputs. Specifically, we can imagine mapping from binary-valued input vectors to a single binary output representing the evaluation of some logical proposition on the input. We train the system to figure out how to compute the predicate from seeing a subset of the (input, output) pairs. \par   
%We can then train the system to do various meta-mappings, for example identifying certain types of functions (like XOR) or negating predicates. \par
%
\section{Learning multivariate polynomials} \label{sec_poly}
\begin{figure}
\centering
\begin{subfigure}[t]{0.5\textwidth}
\includegraphics[width=\textwidth]{figures/poly/basic_results.png}
\caption{The polynomials domain, section \ref{sec_poly}.}
\label{poly_basic_results}
\end{subfigure}%
\begin{subfigure}[t]{0.5\textwidth}
\includegraphics[width=\textwidth]{figures/basic_meta_learning.png}
\caption{The cards domain, section \ref{sec_cards}.}
\label{cards_basic_results}
\end{subfigure}%
\caption{The EML system succeeds at the basic meta-learning tasks, which are a necessary prerequisite for meta-mappings. (\subref{poly_basic_results}) Basic meta-learning in the polynomials domain, section \ref{sec_poly}. The system successfully generalizes to held out polynomials. The solid line indicates optimal performance; the dashed line indicates chance performance (an untrained model). (\subref{cards_basic_results}) Basic meta-learning in the cards domain, section \ref{sec_cards}. The system successfully generalizes to held out games, both when trained on a random sample of half the tasks, or when a targeted subset is held out. The gray dashed line indicates chance performance, while the solid lines are optimal performance. The colored dashed lines show how well the system could perform by playing the strategy from the most correlated trained task. The fact that it generally exceeds this difficult baseline shows a deeper form of generalization than just memorizing a few strategies and picking the closest. Error-bars throughout are bootstrap 95\%-confidence intervals, numerical values for plots can be found in appendix \ref{app_numerical_results}.}
\end{figure}

\begin{figure}
\centering
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=\textwidth]{figures/poly/meta_results.png}
\caption{From examples of mapping.}
\label{poly_meta_map_results_examples}
\end{subfigure}%
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=\textwidth]{figures/poly/language_meta_results.png}
\caption{From language input.}
\label{poly_meta_map_results_language}
\end{subfigure}
\caption{The system generalizes to apply learned meta-mappings to new polynomials, and to apply unlearned meta-mappings, either from examples (\subref{poly_meta_map_results_examples}) or from language cues (\subref{poly_meta_map_results_language}). The plots show the loss produced when evaluating the mapped embedding on the target task. For example, if the initial polynomial is $p(x) = x + 1$, and the meta-task is ``square,'' the loss would be evaluated by applying this meta-mapping to the embedding of $p(x)$ and evaluating how well the function specified by the mapped embedding regresses $p(x)^2 = x^2 + 2x + 1$. The solid line indicates optimal performance; the dashed line is chance (untrained). Error-bars are bootstrap 95\%-CIs.} 
\label{poly_meta_map_results}
\end{figure}
As a proof of concept, we first evaluated the system on the task of learning polynomials of degree $\leq 2$ in 4 variables (i.e. the task was to regress functions of the form $p: \mathbb{R}^4 \rightarrow \mathbb{R}$ where $p \in \mathcal{P}_2 \left(\mathbb{R}\right)$, though the model was given no prior inductive bias toward polynomial forms). This yields an infinite family of base-level tasks (the vector space of all such polynomials), as well as many families of meta-mappings over tasks (for example, multiplying polynomials by a constant, squaring them, or permuting their input variables). This allows us to not only examine the ability of the system to learn to learn polynomials from data, but also to adapt its learned representations in accordance with these meta-tasks. Details of the architecture and training can be found in appendix \ref{app_detailed_methods}.\par
\vspace{-0.7em}
\paragraph{Basic meta learning:} First, we show that the system is able to achieve the basic goal of learning a held-out polynomial from a few data points in Fig. \ref{poly_basic_results} (with good sample-efficiency, see supp. fig. \ref{supp_poly_sweep_results}). \par 
\vspace{-0.7em}
\paragraph{Meta mapping (task $\rightarrow$ task):} Furthermore, the system is able to perform meta-mappings (mappings over tasks) in order to flexibly reconfigure its behavior, as shown in fig. \ref{poly_meta_map_results}. We train the system to perform a variety of mappings, for example switch the first two inputs of the polynomial, add 3 to the polynomial, or square the polynomial. We then test its ability to generalize to held-out mappings, for example a permutation of the polynomial inputs which it has not encountered before, or an additive shift in the polynomial which it has not been trained on. The system is both able to apply its learned meta-mappings to held-out tasks, and to apply meta-mappings it has not been trained on, simply by seeing examples of their application or receiving a natural-language instruction. However, it performs better from examples than language, which is not completely surprising, since the examples provide a richer cue to the task. \par 
\begin{figure}
\centering
\includegraphics[width=\textwidth]{figures/continual/continual_learning.png}
\caption{Once the meta-learning system has been trained on a distribution of prior tasks, its performance on new tasks can be tuned by caching its guessed embeddings for the tasks and then optimizing those, thus avoiding any possibility of interfering with performance on prior tasks. Starting from random embeddings in the trained model results in slower convergence, while in an untrained model the embeddings cannot be optimized well. Error-bars are bootstrap 95\%-CIs.}
\label{poly_continual_results}
\end{figure}
\vspace{-0.7em}
\paragraph{Continual learning:} Although the meta-learning approach is effective for rapidly adapting to a new task, it is unreasonable to think that our system must consider every example it has seen at each inference step. We would like to be able to store our knowledge more efficiently, and allow for further refinement. Furthermore, we would like the system to be able to adapt to new tasks (for which its guessed solution isn't perfect) without catastrophically interfering with prior tasks \citep{McCloskey1989}. \par
A very simple solution to these problems is naturally suggested by our architecture. Specifically, task embeddings can be cached so that they don't have to be regenerated at each inference step. This also allows optimization of these embeddings without altering the other parameters in the architecture, thus allowing fine-tuning on a task without seeing more examples, and without interfering with performance on any other task \citep[cf.][]{Rumelhart1993, Lampinen2018a}. That is, we can see the meta-learning step as a ``warm start'' for an optimization procedure over embeddings that are cached in memory \citep[cf.][]{Kumaran2016}.\footnote{Note that the weights generated by the hyper network could be cached instead of the embeddings, which would be much more memory intensive but would allow even better tuning and would not require passing through the hyper network at inference time.} \par 
To test this idea, we pre-trained the system on 100 polynomial tasks, and then introduced 100 new tasks. We trained on these new tasks by starting from the meta-network's ``guess'' at the correct task embedding, and then optimizing this embedding without altering the other parameters. The results are shown in fig. \ref{poly_continual_results}. The meta-network embeddings offer good immediate performance, and substantially accelerate the optimization process, compared to a randomly-initialized embedding (see supp. fig. \ref{supp_poly_continual_results} for a more direct comparison). Furthermore, this ability to learn is due to training, not simply the expressiveness of the architecture, as is shown by attempting the same with an untrained network. \par 


\section{A stochastic learning setting: simple card games}\label{sec_cards}
We next explored the setting of simple card games, where the agent is dealt a hand and must bet. There is no action besides betting, and depending on the opponent's hand the agent either wins or loses the amount bet. This doesn't require long term planning, but does incorporate some aspects of reinforcement learning, namely stochastic feedback on only the action chosen. We considered five games that are simplified analogs of various real card games (see Appendix \ref{meth_data_cards}). We also considered several binary options that could be applied to the games, including trying to lose instead of trying to win, or switching which suit was more valuable. These are challenging manipulations, for instance trying to lose requires completely inverting a learned $Q$-function. \par
In order to adapt the EML architecture, we made a very simple change. Instead of providing the system with (input, target) tuples to embed, we provided it with (state, action, reward) tuples, and trained it via a DQN-like approach \citep{Mnih2015} to predict rewards for each bet in each state. (A full RL framework is not strictly necessary here because there is no temporal aspect to the tasks; however, we saw it as a useful proof of concept.) The hand is explicitly provided to the network for each example, but which game is being played is implicitly captured in the training examples, without any explicit cues. That is, the system must learn to play directly from seeing a set of (state, action, reward) tuples which implicitly capture the structure and stochasticity of the game. We also trained the system on meta-tasks, such as to identifying the various types of games and options, and to make meta-mappings, for example switching from trying to win a game to trying to lose. Details of the architecture and training can be found in appendix \ref{app_detailed_methods}. \par
\vspace{-0.7em}
\paragraph{Basic meta learning:} First, we show that the system is able to achieve the basic goal of playing a held-out game from examples in Fig. \ref{cards_basic_results}. We compare two different possible hold-out sets: 1) train on half the tasks at random, or 2) specifically hold out all the ``losers'' variations of the ``straight flush'' game. In either of these cases, the meta-learning system achieves well above chance performance (0) at the held out tasks, although it is slightly worse at generalizing to the targeted hold out, despite having more training tasks in that case. Note that the sample complexity in terms of number of trained tasks is not that high, even training on 20 randomly selected tasks leads to good generalization to the held-out tasks. Furthermore, the task embeddings generated by the system are organized by the features of the games, see appendix \ref{app_cards_tsne}. \par
\vspace{-0.7em}
\paragraph{Meta mapping (task $\rightarrow$ task):} Furthermore, the system is able to perform meta-mappings (mappings over tasks) in order to flexibly reconfigure its behavior. For example, if the system is trained to map games to their losers variations, it can generalize this mapping to a game it has not been trained to map, even if the source or target of that mapping is held out from training. In Fig. \ref{cards_meta_map_results}) we demonstrate this by taking the mapped embedding and evaluating the reward received by playing the targeted game with it. This task is more difficult than simply learning to play a held out game from examples, because the system will actually receive no examples of the target game (when it is held out). Furthermore, in the case of the losers mapping, leaving the strategy unchanged would produce a large negative reward, and chance performance would produce 0 reward, so the results are quite good. \par
\begin{figure}
\centering
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=\textwidth]{figures/meta_mapping.png}
\caption{From examples of mapping.}
\label{cards_meta_map_results_examples}
\end{subfigure}%
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=\textwidth]{figures/language_meta_mapping.png}
\caption{From language input.}
\label{cards_meta_map_results_language}
\end{subfigure}
\caption{The system generalizes to meta-mapping new tasks in the cards domain. The system is trained to do the meta-mappings shown here on a subset of its basic tasks, and is able to generalize these mappings to novel tasks it has not been trained on. For example, for the ``losers'' mapping, the sytem is trained to map games to their losers variants. When given a novel game, it is able to apply this mapping to guess how to play the losers variation. This plot shows the reward produced by taking the mapped embedding and playing the targeted game. The gray dashed line indicates random performance, while the colored dashed lines indicate performance if the system did not alter its behavior in response to the meta-mapping. The system generally exceeds these baselines, although the switch-suits baseline is more difficult with the targeted holdout. The meta-mappings can be cued either from examples (\subref{cards_meta_map_results_examples}) or from language cues (\subref{cards_meta_map_results_language}). Error-bars are bootstrap 95\%-CIs.}
\label{cards_meta_map_results}
\end{figure}
\section{Discussion}
\paragraph{Related work:} Our work is an extrapolation from the rapidly-growing literature on meta-learning \citep[e.g.][]{Vinyals2016, Santoro2016, Finn2017a, Finn2018, Stadie2018}. It is also related to the literature on continual learning, or more generally tools for avoiding catastrophic interference based on changes to the architecture \citep[e.g.][]{Fernando2017, Rusu2016}, loss \citep[e.g.][]{Kirkpatrick2016, Zenke2017, Aljundi2019}, or external memory \citep[e.g.][]{Sprechmann2018}. Recent work has also begun to blur the separation between these approaches, for example by meta-learning in an online setting \citep{Finn2019}. Our work is specifically inspired by the algorithms that attempt to have the system learn to adapt to a new task via activations rather than weight updates, either from examples \citep[e.g.][]{Wang2016a, Duan2016}, or a task input \citep[e.g.][]{Borsa2019}. \par
Our architecture builds directly off of prior work on HyperNetworks \citep{Ha2016} -- networks which parameterize other networks -- and other recent applications thereof, such as guessing parameters for a model to accelerate model search \citep[e.g.][]{Brock2018a, Zhang2019}. This is related to the longer history of work on different time-scales of weight adaptation \citep{Hinton1982, Kumaran2016} that has more recently been applied to meta-learning contexts \citep[e.g.][]{Ba2016, Munkhdalai2017} and continual learning \citep[e.g.]{Hu2019}. It is more abstractly related to work on learning to propose architectures \citep[e.g.][]{Zoph2016, Cao2019}, and to models that learn to select and compose skills to apply to new tasks \citep[e.g.][]{Andreas, Andreas2016, Tessler2016, Reed2015, Chang2019a}. In particular, some of the work in domains like visual question answering has explicitly explored the idea of building a classifier conditioned on a question \citep{Andreas, Andreasa}, which is related to one of the possible computational paths through our architecture. Work in model-based reinforcement learning has also partly addressed how to transfer knowledge between different reward functions \citep[e.g.][]{Laroche2017}; our approach is more general. \par 
There has also been other recent interest in task (or function) embeddings. In simultaneous work Achille et al. \citep{Achille2019} proposed computing embeddings for visual tasks from the Fisher information of the parameters in a model partly tuned on the task. They show that this captures some interesting properties of the tasks, including some types of semantic relationships, and can help identify models that can perform well on a task. Rusu and colleagues recently suggested a similar meta-learning framework where latent codes are computed for a task which can be decoded to a distribution over parameters \citep{Rusu2019}. Other recent work has tried to learn representations for skills \citep[e.g.][]{Eysenbach2019} or tasks \citep[e.g.]{Hsu2019} for exploration and representation learning. Our perspective can be seen as a generalization of these that learns task embeddings in an end-to-end way, allows parameterizing a model (while some approaches only allow selection), but most importantly allows for remapping of behavior by meta-tasks. To the best of our knowledge none of the prior work has explored meta-mappings. \par
\vspace{-1em}
\paragraph{Future Directions:} We think that the general perspective of embedding tasks and considering meta-mappings will yield many fruitful future directions. We hope that our work will inspire more exploration of behavioral adaptation in areas beyond the simple domains we considered here. To this end, we suggest the creation of meta-learning datasets which include information not only about tasks, but about the relationships between them. For example, as we alluded to above, tasks such as visual question-answering \citep[e.g.][]{Antol2015} can be usefully interpreted from this perspective (a question essentially yields a function from images to answers, and there are many systematic relationships between these functions), as can reinforcement learning tasks which involve executing instructions \citep[e.g.][]{Hermann2017, Co-Reyes2019}. Furthermore, we think our work provides a novel perspective on the types of flexibility that human intelligence exhibits, and thus hope that it may have implications for cognitive science. \par 
We do not necessarily believe that the particular architecture we have suggested is the best architecture for addressing these problems generally, although it has a number of desirable characteristics. However, because of the way the architecture is modularized, it is relatively easy to change any aspect of it without altering the others. (Note that we compare some variations in appendix \ref{app_lesion_results}.) For example, although we only considered task networks $F$ that are feed-forward and of a fixed depth, this could be replaced with a recurrent architecture to allow more adaptive computation, or even a more complex architecture \citep[e.g.][]{Reed2015, Graves2016}. Our work also opens the possibility of doing unsupervised learning over function representations for further learning, which relates to long-standing ideas in cognitive science about how humans represent knowledge \citep{Clark1993}. \par 
%\vspace{-1em}
%\paragraph{Conclusions:}
\section{Conclusions}
In this paper, we've highlighted a new type of flexibility in the form of \emph{meta-mappings}, that is, mappings between tasks. We've suggested that achieving this type of flexibility is difficult in most deep learning architectures, because they separate computation from the knowledge of what is being computed. To address this, we've proposed explicitly representing the tasks by embeddings, and deriving the computation from these embeddings via a HyperNetwork. This allows transformations over task embeddings to transform the system's behavior. To this end, we've proposed a meta-learning system which embeds tasks and data into a shared latent space, and then meta-learns to make mappings over this space. The requisite meta-task sample complexity is small; we showed generalization to unseen meta-mappings after training on only 20 meta-mappings.\par
We see our proposal as a logical progression from the fundamental idea of meta-learning -- that there is a continuum between data and tasks. This naturally leads to the idea of manipulating task representations just like we manipulate data. We've shown that this approach yields considerable flexibility, most importantly the meta-mapping ability to adapt zero-shot to a new task. We've also shown that our approach provides a useful perspective on continual learning. We hope that these results will lead to the development of more powerful and flexible deep-learning models. \par
%\subsubsection*{Acknowledgements}
%We would like to acknowledge Noah Goodman, Surya Ganguli, Katherine Hermann, Erin Bennett, and Arianna Yuan for stimulating questions and suggestions on this project.

\bibliographystyle{acm}
\bibliography{arrr}

\newpage
% comment the following to compile without appendix but with references
\include{embedded_meta_learning_appendix}


\end{document}


