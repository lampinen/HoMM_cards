\appendix
The supplemental material is organized as follows: In section \ref{app_clarifying_meta_mapping} we clarify some definitional details, and discuss the value of meta-mappings in detail by comparing to other methods of performing a new task. In section \ref{app_cards_tsne} we show $t$-sne results for the cards domain. In section \ref{app_supp_figures} we provide supplemental figures. In section \ref{app_lesion_results} we provide some lesion studies. In section \ref{app_detailed_methods} we list details fo the datasets and architectures we used, as well as providing links to the source code for all models, experiments, and analyses. In section \ref{app_numerical_results} we provide means and bootstrap CIs corresponding to the major figures in the paper. \par  

\section{Clarifying meta-mappings} \label{app_clarifying_meta_mapping}
\subsection{Why meta-map from tasks to tasks?} \label{app_why_meta_mapping}
Why are meta-mappings between tasks useful? To answer this, we consider various ways of adapting to a new task in figure \ref{fig_why_meta_mapping} (based on results from the cards domain, section \ref{sec_cards}). The system could adapt from seeing examples of the new task, but this requires going out and collecting data, which may be expensive and does not allow zero-shot adaptation. Alternatively, the system could perform the new task via a meta-mapping from a prior learned task, where the meta-mapping is either induced from examples of the meta-mapping, or from language. Finally, the system could perform a new task from language alone, if it is trained to map instructions to tasks. \par
To address this latter possibility, we trained a version of the model where we included training the language system to produce embeddings for the basic tasks (while simultaneously training the system on all the other objectives, such as performing the tasks from examples, in order to provide the strongest possible structuring of the system's knowledge for the strongest possible comparison). We compare this model's performance at held-out tasks to that of systems learning from examples of the new task directly, or from meta-mapping, see fig. \ref{fig_why_meta_mapping}. \par 
These results demonstrate the advantage of meta-mapping. While learning from examples is still better given enough data, it requires potentially-expensive data collection and does not allow zero-shot adaptation. Performing the new task from a language description alone uses only the implicit knowledge in the model's weights, and likely because of this it does not generalize well to the difficult held-out tasks. Meta-mapping performs substantially better, while relying only on cached prior knowledge, viz. prior task-embedding(s) and a description of the meta-mapping (either in the form of examples or natural language). That is, meta-mapping has the advantage of requiring no new data collection, like performing from language alone, but results in much better performance by leveraging a richer description of the new task constructed using the system's knowledge of a prior task and the new task's relationship to it.\par 
\subsection{A definitional note}
When we discussed meta-mappings in the main task, we equivocated between tasks and behaviors for the sake of brevity. For a perfect model, this is somewhat justifiable, because each task will have a corresponding optimal behavior, and the sytem's embedding of the task will be precisely the embedding which produces this optimal behavior. However, behavior-irrelevant details of the task, like the color of the board, may not be embedded, so this should not really be thought of as a task-to-task mapping. This problem is exacerbated when the system is imperfect, e.g. during learning. It is thus more precise to distinguish between a ground-truth meta-mapping, which maps tasks to tasks, and the computational approach to achieving that meta-mapping, which really maps between representations which combine both task and behavior. \par
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/why_meta_mapping.png}
\caption{Comparison of a variety of methods for performing one of the 10\% held-out tasks in the more difficult hold-out set in the cards domain. There are a number of ways the system could adapt to a new task: from seeing example of the new task, from hearing the new task described from language alone, or from leveraging its knowledge about prior tasks via meta-mappings (in this case, from the non-losers variations of the same games). The meta-mappings offer a happy medium between the other two alternatives -- they only require cached knowledge of prior tasks, rather than needing to collect experience on the task before a policy can be derived, but they outperform a system that simply tries to construct the task embedding from a description alone. Language alone is not nearly as rich a cue as knowledge of how a new task relates to prior tasks.} 
\label{fig_why_meta_mapping}
\end{figure}


\section{Supplemental figures} \label{app_supp_figures}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/poly/basic_results_sweeping.png}
\caption{The system is able to infer polynomials from only seeing a few data points (i.e. evaluations of the polynomial), despite the fact that during training it always saw 50. A minimum of 15 random points is needed to correctly infer polynomials without prior knowledge of the polynomial distribution, but the system is performing well below this value, and quite well above it, although it continues to refine its estimates slightly when given more data.}
\label{supp_poly_sweep_results}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/poly/base_learning_curves.png}
\caption{Learning curves for basic regression in the polynomials domain.}
\label{supp_poly_basic_learning_curves}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/poly/meta_learning_curves.png}
\caption{Learning curves for meta-mappings in the polynomials domain. Although the results seem to be leveling off at the end, we found that generalization performance was slightly increasing or stable in this region, which may have interesting implications about the structure of these tasks \citep{Lampinen2019}.}
\label{supp_poly_meta_learning_curves}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/continual/continual_learning_direct_comparison.png}
\caption{Continual learning in the polynomials domain: a more direct comparison. Once the meta-learning system has been trained on a distribution of prior tasks, its performance on new tasks can be tuned by caching its guessed embeddings for the tasks and then optimizing those, thus avoiding any possibility of interfering with performance on prior tasks. Starting with the guessed embedding substantially speeds-up the process compared to a randomly-initialized embedding. Furthermore, this ability to learn is due to training, not simply the expressiveness of the architecture, as is shown by attempting the same with an untrained network.}
\label{supp_poly_continual_results}
\end{figure}

\begin{figure}[H]
\centering
\begin{subfigure}[t]{0.5\textwidth}
\includegraphics[width=\textwidth]{figures/poly/basic_results_over_time.png}
\caption{The polynomials domain, section \ref{sec_poly}.}
\label{supp_poly_integration_results}
\end{subfigure}%
\begin{subfigure}[t]{0.5\textwidth}
\includegraphics[width=\textwidth]{figures/basic_meta_learning_integrating_over_time.png}
\caption{The cards domain, section \ref{sec_cards}.}
\label{supp_cards_integration_results}
\end{subfigure}%
\caption{Integrating new tasks into the system by training all parameters results in some initial interference with prior tasks (even with replay), suggesting that an approach like the continual learning-approach may be useful.}
\label{supp_integration_results}
\end{figure}

\section{Card game $t$-SNE} \label{app_cards_tsne}
We performed $t$-SNE \citep{LaurensvanderMaaten2008} on the task embeddings of the system at the end of learning the card game tasks, to evaluate the organization of knowledge in the network. In fig. \ref{fig_cards_tsne_basic} we show these embeddings for just the basic tasks. The embeddings show systematic grouping by game attributes. In fig. \ref{fig_cards_tsne_full} we show the embeddings of the meta and basic tasks, showing the organization of the meta-tasks by type. \par 
\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/basic_tsne_basic_final.png}
\caption{$t$-SNE embedding of the function embeddings the system learned for the basic card game tasks. (Note that the pairs of nearby embeddings differ in the ``suits rule`` attribute, discussed in appendix \ref{meth_data_cards}.)} 
\label{fig_cards_tsne_basic}
\end{figure}%
\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{figures/basic_tsne_full_final.png}
\caption{$t$-SNE embedding of the function embeddings the system learned for the meta tasks (basic tasks are included in the background).} 
\label{fig_cards_tsne_full}
\end{figure}

\section{Lesion experiments} \label{app_lesion_results}
In this section we consider a few variations of the architecture, to justify the choices made in the paper. \par

\subsection{Shared $Z$ vs. separate task-embedding and data-embedding space} \label{app_lesion_results_shared_z}
Instead of having a shared $Z$ where data and tasks are embedded, why not have a separate embedding space for data, tasks, and so on? There are a few conceptual reason why we chose to have a shared $Z$, including its greater parameter efficiency, the fact that humans seem to represent our conscious knowledge of different kinds in a shared space \citep[][]{Baars2005}, and the fact that this representation could allow for zero-shot adaptation to new computational pathways through the latent space, analogously to the zero-shot language translation results reported by Johnson and colleagues \citep{Johnson2016a}. In this section, we further show that training with a separate task encoding space worsens performance, see fig. \ref{supp_lesion_shared_z_fig}. This seems to primarily be due to the fact that learning in the shared $Z$ accelerates and de-noises the learning process, see fig. \ref{supp_lesion_shared_z_learn_fig}. (It's therefore worth noting that running this model for longer could result in convergence to the same asymptotic generalization performance.) \par
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/poly/meta_results_unshared_vs_shared.png}
\caption{Having a separate embedding space for tasks results in worse performance on meta-mappings. (Results are from only 1 run.)}
\label{supp_lesion_shared_z_fig}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/poly/meta_learning_curves_with_separate.png}
\caption{Having a separate embedding space for tasks results in noisier, slower learning of meta-mappings. (Results are from only 1 run.)}
\label{supp_lesion_shared_z_learn_fig}
\end{figure}

\subsection{Hyper network vs. conditioned task network} \label{app_lesion_results_hyper}
Instead of having the task network $F$ parameterized by the hyper network $\mathcal{H}$, we could simply have a task network with learned weights which takes a task embedding as another input. Here, we show that this architecture fails to learn the meta-mapping tasks, although it can successfully perform the basic tasks. We suggest that this is because it is harder for this architecture to prevent interference between the comparatively larger number of basic tasks and the smaller number of meta-tasks. While it might be possible to succeed with this architecture, it was more difficult in the hyper-parameter space we searched.\par 
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{figures/poly/meta_results_conditioned_vs_hyper.png}
\caption{Conditioning the task network on the task embedding, rather than parameterizing it via a hyper network causes it to fail at the meta-mapping tasks. Results are from only 2 runs.}
\label{supp_lesion_hyper}
\end{figure}

\section{Detailed methods} \label{app_detailed_methods}
\subsection{Datasets}
\subsubsection{Polynomials} \label{meth_data_poly}
We randomly sampled the train and test polynomials as follows:
\begin{enumerate}
\item Sample the number of relevant variables ($k$) uniformly at random from 0 (i.e. a constant) to the total number of variables.
\item Sample the subset of $k$ variables that are relevant from all the variables.
\item For each term combining the relevant variables (including the intercept), include the term with probability 0.5. If so give it a random coefficient drawn from $\mathcal{N}(0, 2.5)$.
\end{enumerate}
The data points on which these polynomials were evaluated were sampled uniformly from $[-1, 1]$ independently for each variable, and for each polynomial. The datasets were resampled every 50 epochs of training. \par
\textbf{Meta-tasks:} For meta-tasks, we trained the network on 6 task-embedding classification tasks:
\begin{itemize}
\item Classifying polynomials as constant/non-constant.
\item Classifying polynomials as zero/non-zero intercept.
\item For each variable, identifying whether that variable was relevant to the polynomial.
\end{itemize}
We trained on 20 meta-mapping tasks, and held out 16 related meta-mappings.
\begin{itemize}
\item Squaring polynomials (where applicable).
\item Adding a constant (trained: -3, -1, 1, 3, held-out: 2, -2).
\item Multiplying by a constant (trained: -3, -1, 3, held-out: 2, -2).
\item Permuting inputs (trained: 1320, 1302, 3201, 2103, 3102, 0132, 2031, 3210, 2301, 1203, 1023, 2310, held-out: 0312, 0213, 0321, 3012, 1230, 1032, 3021, 0231, 0123, 3120, 2130, 2013).
\end{itemize}
\textbf{Language:} We encoded the meta-tasks in language by sequences as follows:
\begin{itemize}
\item Classifying polynomials as constant/non-constant: \verb|[``is'', ``constant'']|
\item Classifying polynomials as zero/non-zero intercept: \verb|[``is'', ``intercept_nonzero'']|
\item For each variable, identifying whether that variable was relevant to the polynomial: \verb|[``is'', <variable-name>, ``relevant'']|
\item Squaring polynomials: \verb|[``square'']|
\item Adding a constant: \verb|[``add'', <value>]|
\item Multiplying by a constant: \verb|[``multiply'', <value>]|
\item Permuting inputs: 
\begin{verbatim}[``permute'', <variable-name>, <variable-name>, <variable-name>, 
<variable-name>]\end{verbatim}
\end{itemize}
All sequences were front-padded with ``<PAD>'' to the length of the longest sequence.

\subsubsection{Card games}
\label{meth_data_cards}
Our card games were played with two suits, and 4 values per suit. In our setup, each hand in a game has a win probability (proportional to how it ranks against all other possible hands). The agent is dealt a hand, and then has to choose to bet 0, 1, or 2 (the three actions it has available). We considered a variety of games which depend on different features of the hand: 
\begin{itemize}
\item \textbf{High card:} Highest card wins.
\item \textbf{Pairs} Same as high card, except pairs are more valuable, and same suit pairs are even more valuable.
\item \textbf{Straight flush:} Most valuable is adjacent numbers in same suit, i.e. 4 and 3 in most valuable suit wins every time (royal flush).
\item \textbf{Match:} the hand with cards that differ least in value (suit counts as 0.5 pt difference) wins.
\item \textbf{Blackjack:} The hand's value increases with the sum of the cards until it crosses 5, at which point the player ``goes bust,'' and the value becomes negative. 
\end{itemize}
We also considered three binary attributes that could be altered to produce variants of these games:
\begin{itemize}
\item \textbf{Losers:} Try to lose instead of winning! Reverses the ranking of hands.
\item \textbf{Suits rule:} Instead of suits being less important than values, they are more important (essentially flipping the role of suit and value in most games).
\item \textbf{Switch suit:} Switches which of the suits is more valuable.
\end{itemize}
Any combination of these options can be applied to any of the 5 games, yielding 40 possible games. The systems were trained with the full 40 possible games, but after training we discovered that the ``suits rule'' option does not substantially alter the games we chose (in the sense that the probability of a hand winning in the two variants of a game is very highly correlated), so we have omitted it from our analyses.\par
\textbf{Meta-tasks:} For meta-tasks, we gave the network 8 task-embedding classification tasks (one-vs-all classification of each of the 5 game types, and of each of the 3 attributes), and 3 meta-mapping tasks (each of the 3 attributes). \par
\textbf{Language:} We encoded the meta-tasks in language by sequences of the form \verb|[``toggle'', <attribute-name>]| for the meta-mapping tasks, and \verb|[``is'', <attribute-or-game-name>]|.
\subsection{Model \& training} \label{app_model_details}
\textbf{Basic task operation:}
\begin{enumerate}
\item A training dataset $D_1$ of (input, target) pairs is embedded by $\mathcal{I}$ and $\mathcal{T}$ to produce a set of paired embeddings. Another set of (possibly unlabeled) inputs $D_2$ is provided and embedded.
\item The meta network $\mathcal{M}$ maps the set of embedded (input, target) pairs to a function embedding.
\item The hyper network $\mathcal{H}$ maps the function embedding to parameters for $F$, which is used to transform the second set of inputs to a set of output embeddings.
\item The output embeddings are decoded by $\mathcal{O}$ to produce a set of outputs.
\item The system is trained end-to-end to minimize the loss on these outputs.
\end{enumerate}
The model is trained to minimize 
\[\mathbb{E}_{(x, y)\in {D}_2} \left[ \mathfrak{L}\left(y, \mathcal{O}\left(F_{D_1}\left(\mathcal{I} \left(x\right)\right) \right)\right)\right]\]
where $F_{D_1}$ is the transformation the meta-learner guesses for the training dataset $D_1$:
\[F_{D_1} \text{ is parameterized by } \mathcal{H}\left(\mathcal{M}\left( \left\{\left(\mathcal{I}\left(x_i\right), \mathcal{T}\left(y_i\right) \right) \text{ for } (x_i, y_i) \in D_1 \right\}\right)\right)\]
\textbf{Meta-task operation:}
\begin{enumerate}
\item A meta-dataset of (source-task-embedding, target-task-embedding) pairs, $D_1$, is collected. Another dataset $D_2$ (possibly only source tasks) is provided.
\item The meta network $\mathcal{M}$ maps this set of (source, target) task-embedding pairs to a function embedding.
\item The hyper network $\mathcal{H}$ maps the function embedding to parameters for $F$, which is used to transform the second set of inputs to a set of output embeddings.
\item The system is trained to minimize $\ell_2$ loss between these mapped embeddings and the target embeddings. 
\end{enumerate}
The model is trained to minimize
\[\mathbb{E}_{(z_{source}, z_{target})\in {D}_2} \left[ \mathfrak{L}\left(z_{target}, F_{D_1}\left(\mathcal{I} \left(z_{source}\right)\right) \right)\right]\]
where $\mathfrak{L}$ is $\ell_2$ loss, and $F_{D_1}$ is the transformation the meta-learner guesses for the training dataset $D_1$:
\[F_{D_1} \text{ is parameterized by } \mathcal{H}\left(\mathcal{M}\left( \left\{\left((z_{source},z_{target}\right) \in D_1 \right\}\right)\right)\]
Note that there are two kinds of hold-out in the training of this system. First, some of the trained tasks are held-out on each training step (i.e. $D_2$), in order to force the system to generalize rather than just memorize. Second, there are truly held-out tasks that the system has never seen in training, these are the held-out tasks and meta-mappings that we evaluate on at the end of training. \par
\textbf{Language-cued meta-tasks:} The procedure is analogous to the meta-tasks from examples, except that the input to $\mathcal{H}$ is the embedding of the language input, rather than the output of $\mathcal{M}$. The systems that were trained from language were also trained with the example-based meta-tasks.\par



\subsubsection{Detailed hyper-parameters}
\begin{table}
\centering
\begin{tabular}{|c||c|c|c|}
\hline 
& \phantom{bla}Polynomials\phantom{la} & Continual learning & \phantom{blahb}Cards\phantom{blahb} \\\hline
\hline
$Z$-dimension & 512 & 512 & 512 \\\hline
$\mathcal{I}$ num. layers & \multicolumn{3}{c|}{3} \\\hline 
$\mathcal{I}$ num. hidden units & \multicolumn{3}{c|}{64} \\\hline 
$\mathcal{L}$ architecture & \multicolumn{1}{p{2.5cm}|}{2-layer LSTM + 2 fully-connected} & - & \multicolumn{1}{p{2.5cm}|}{1-layer LSTM + 2 fully-connected} \\\hline 
$\mathcal{L}$ num. hidden units & 512 & - & 512 \\\hline 
$\mathcal{O}$ num. layers & 1 & 1 & 3 \\\hline 
$\mathcal{O}$ num. hidden units & - & - & 512 \\\hline 
$\mathcal{T}$ num. layers & \multicolumn{3}{c|}{1} \\\hline 
$\mathcal{M}$ architecture & \multicolumn{3}{c|}{2 layers per-datum, max pool across, 2 layers} \\\hline 
$\mathcal{H}$ architecture & \multicolumn{3}{c|}{4 layers} \\\hline 
$\mathcal{M}, \mathcal{H}$ num. hidden units & \multicolumn{3}{c|}{512} \\\hline 
$F$ architecture & \multicolumn{3}{c|}{4 layers} \\\hline 
$F$ num. hidden units & \multicolumn{3}{c|}{64} \\\hline 
Nonlinearities & \multicolumn{3}{p{8.5cm}|}{Leaky ReLU in most places, except no non-linearity at final layer of $\mathcal{T}$, $\mathcal{M}$, $\mathcal{L}$, $F$, and sigmoid for meta-classification outputs.} \\\hline
Main loss & \multicolumn{3}{p{8.5cm}|}{$\ell_2$ for main task \& meta-mapping, cross-entropy for meta-classification.}\\\hline
\hline
Optimizer & Adam & RMSProp & RMSProp \\\hline
Learning rate (base) & $3\cdot 10^{-5}$ & $1\cdot 10^{-4}$ & $1\cdot 10^{-4}$\\\hline
Learning rate (meta) & $1\cdot 10^{-5}$ & - & $1\cdot 10^{-4}$\\\hline
L.R. decay rate (base) & $*0.85$ & $*0.85$ & $*0.85$ \\\hline
L.R. decay rate (meta) & $*0.85$ & - & $*0.9$ \\\hline
L.R. min (base) & \multicolumn{3}{c|}{$3 \cdot 10^{-8}$} \\\hline
L.R. min (meta) & $1 \cdot 10^{-7}$& - & $3 \cdot 10^{-7}$ \\\hline
L.R. decays every & \multicolumn{3}{c|}{$100$ epochs if above min.} \\\hline
Cached embedding L.R. & - & $1\cdot 10^{-3}$ & -\\\hline
Num. training epochs & 4000 & 3000 & 40000 \\\hline
Num. runs & 5 & 5 & 10 \\ \hline
\hline
Num. base tasks (eval) & 60 & 100 & 36 or 20 \\\hline
Num. base tasks (training) & 1200 (= 60 * 20) & 100 & 36 or 20  \\\hline
Num. meta classifications & 6 & - & 8  \\\hline
Num. meta mappings & 20 & - & 3  \\\hline
Num. new base tasks & 40 & 30 & 4 or 20 \\\hline 
Num. new meta mappings & 16 & - & 0  \\\hline
Num. new meta classifications &  \multicolumn{3}{c|}{0} \\\hline
Base dataset size & \multicolumn{3}{c|}{1024} \\\hline
Base datasets refreshed & \multicolumn{3}{c|}{Every 50 epochs} \\\hline
$\mathcal{M}$ batch size & 50 & 128 & 768 \\\hline
\end{tabular}
\caption{Detailed hyperparameter specification for different experiments. A ``-'' indicates a parameter that does not apply to that experiment. Where only one value is given, it applied to all the experiments discussed. As a reminder: the shared representational space is denoted by $Z$. Input encoder: $\mathcal{I}: \text{input} \rightarrow Z$. Output decoder $\mathcal{O}: Z \rightarrow \text{output}$. Target encoder $\mathcal{T}: \text{targets} \rightarrow Z$. Meta-network $\mathcal{M}: \{(Z, Z), ...\} \rightarrow Z $ -- takes a set of (input embedding, target embedding) pairs and produces a function embedding. Hyper-network $\mathcal{h}: Z \rightarrow \text{parameters}$ -- takes a function embedding and produces a set of parameters. Task network $F: Z \rightarrow Z$ -- the transformation that executes the task mapping, implemented by a deep network with parameters specified by $\mathcal{H}$. Where language was used to cue meta-mappings, it was processed by language encoder: $\mathcal{L}: \text{natural language} \rightarrow Z$. } \label{supp_hyperparameter_table}
\end{table}
See table \ref{supp_hyperparameter_table} for detailed architectural description and hyperparameters for each experiment. Hyperparameters were generally found by a heuristic search, where mostly only the optimizer, learning rate annealing schedule, and number of training epochs were varied, not the architectural parameters. Some of the parameters take the values they do for fairly arbitrary reasons, e.g. the continual learning experiments were run with the current polynomial hyperparameters before the hyperparameter search for the polynomial data was complete, so some parameters are altered between these. \par
Each epoch consisted of a separate learning step on each task (both base and meta), in a random order. In each task, the meta-learner would receive only a subset (the ``batch size`` above) of the examples to generate a function embedding, and would have to generalize to the remainder of the examples in the dataset. The embeddings of the tasks for the meta-learner were computed once per epoch, so as the network learned over the course of the epoch, these embeddings would get ``stale,'' but this did not seem to be too detrimental. \par 
The results reported in the figures in this paper are averages across multiple runs, with different trained and held-out tasks (in the polynomial case) and different network initializations (in all cases), to ensure the robustness of the findings. \par 

\subsection{Source repositories}
The full code for the experiments and analyses can be found on github:
%%\begin{itemize}
%%\item \url{https://github.com/lampinen/polynomials}
%%    \begin{itemize}
%%        \item See the \verb|continual_learning| and \verb|language_meta_only| branches for the continual learning and language results, respectively. 
%%    \end{itemize}
%%\item \url{https://github.com/lampinen/meta_cards} 
%%\end{itemize}
(will be made available in the de-anonymized version.)
\section{Numerical results} \label{app_numerical_results}
In this section we provide the mean values and bootstrap confidence intervals corresponding to the major figures in the paper, as well as the baseline results in those figures. Tables were generated with stargazer \citep{Hlavac2018}. \par

\subsection{Polynomials}
\begin{table}[H]
\scriptsize
\centering
\begin{tabular}{@{\extracolsep{5pt}} ccccc}
\\[-1.8ex]\hline
\hline \\[-1.8ex]
named\_run\_type & is\_new & mean\_loss & boot\_CI\_low & boot\_CI\_high \\
\hline \\[-1.8ex]
EML & Trained & 0.015 & 0.012 & 0.018 \\
EML & Held out & 0.246 & 0.188 & 0.308 \\
Untrained EML network & Trained & 5.735 & 4.823 & 6.74 \\
Untrained EML network & Held out & 5.968 & 4.984 & 6.991 \\
\hline \\[-1.8ex]
\end{tabular}
\caption{Table for basic meta-learning, figure \ref{poly_basic_results}}
\end{table}

\begin{table}[H]
\scriptsize
\centering
\begin{tabular}{@{\extracolsep{5pt}} ccccc}
\\[-1.8ex]\hline
\hline \\[-1.8ex]
named\_run\_type & result\_type & mean\_loss & boot\_CI\_low & boot\_CI\_high \\
\hline \\[-1.8ex]
EML & Trained mapping, on trained task & 0.094 & 0.091 & 0.098 \\
EML & Trained mapping, on held-out task & 1.721 & 1.419 & 2.115 \\
EML & Held-out mapping, on trained task & 1.28 & 1.213 & 1.35 \\
EML & Held-out mapping, on held-out task & 1.775 & 1.706 & 1.846 \\
Untrained EML network & Trained mapping, on trained task & 12.998 & 11.689 & 14.381 \\
Untrained EML network & Trained mapping, on held-out task & 15.002 & 13.39 & 16.83 \\
Untrained EML network & Held-out mapping, on trained task & 8.36 & 7.898 & 8.833 \\
Untrained EML network & Held-out mapping, on held-out task & 8.786 & 8.317 & 9.27 \\
\hline \\[-1.8ex]
\end{tabular}
\caption{Table for meta-mapping results from examples, figure \ref{poly_meta_map_results_examples}}
\end{table}

\begin{table}[H]
\scriptsize
\centering
\begin{tabular}{@{\extracolsep{5pt}} ccccc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
named\_run\_type & result\_type & mean\_loss & boot\_CI\_low & boot\_CI\_high \\ 
\hline \\[-1.8ex] 
Language & Trained mapping, on trained task & 0.515 & 0.483 & 0.552 \\ 
Language & Trained mapping, on held-out task & 2.244 & 1.921 & 2.623 \\ 
Language & Held-out mapping, on trained task & 2.072 & 1.958 & 2.19 \\ 
Language & Held-out mapping, on held-out task & 2.35 & 2.254 & 2.447 \\ 
Untrained EML network & Trained mapping, on trained task & 13.328 & 11.977 & 14.823 \\ 
Untrained EML network & Trained mapping, on held-out task & 15.313 & 13.602 & 17.354 \\ 
Untrained EML network & Held-out mapping, on trained task & 8.205 & 7.795 & 8.662 \\ 
Untrained EML network & Held-out mapping, on held-out task & 8.625 & 8.131 & 9.104 \\ 
\hline \\[-1.8ex] 
\end{tabular}
\caption{Table for meta-mapping results from language, figure \ref{poly_meta_map_results_language}}
\end{table}

\subsection{Cards}
\begin{table}[H]
\scriptsize
\centering
\begin{tabular}{@{\extracolsep{5pt}} cccccc}
\\[-1.8ex]\hline
\hline \\[-1.8ex]
named\_run\_type & named\_game\_type & is\_new\_game & average\_reward & avg\_rwd\_CI\_low & avg\_rwd\_CI\_high \\
\hline \\[-1.8ex]
Random 50\% holdout & High card & Trained & 0.53 & 0.521 & 0.541 \\
Random 50\% holdout & High card & Held out & 0.441 & 0.42 & 0.462 \\
Random 50\% holdout & Match & Trained & 0.537 & 0.524 & 0.55 \\
Random 50\% holdout & Match & Held out & 0.539 & 0.523 & 0.556 \\
Random 50\% holdout & Pairs & Trained & 0.521 & 0.504 & 0.536 \\
Random 50\% holdout & Pairs & Held out & 0.453 & 0.434 & 0.47 \\
Random 50\% holdout & Straight flush & Trained & 0.525 & 0.508 & 0.54 \\
Random 50\% holdout & Straight flush & Held out & 0.484 & 0.466 & 0.502 \\
Random 50\% holdout & Blackjack & Trained & 0.582 & 0.557 & 0.603 \\
Random 50\% holdout & Blackjack & Held out & 0.492 & 0.468 & 0.513 \\
Targeted 10\% holdout & High card & Trained & 0.527 & 0.518 & 0.536 \\
Targeted 10\% holdout & Match & Trained & 0.536 & 0.526 & 0.546 \\
Targeted 10\% holdout & Pairs & Trained & 0.522 & 0.512 & 0.531 \\
Targeted 10\% holdout & Straight flush & Trained & 0.524 & 0.509 & 0.538 \\
Targeted 10\% holdout & Straight flush & Held out & 0.361 & 0.332 & 0.39 \\
Targeted 10\% holdout & Blackjack & Trained & 0.586 & 0.575 & 0.598 \\
\hline \\[-1.8ex]
\end{tabular}
\caption{Table for basic meta-learning, figure \ref{cards_basic_results}}
\end{table}

\begin{table}[H]
\scriptsize
\centering
\begin{tabular}{@{\extracolsep{5pt}} cccc}
\\[-1.8ex]\hline
\hline \\[-1.8ex]
is\_new\_game & named\_run\_type & named\_game\_type & expected\_reward \\
\hline \\[-1.8ex]
Trained & Targeted 10\% holdout & High card & 0.531 \\
Trained & Targeted 10\% holdout & Match & 0.541 \\
Trained & Targeted 10\% holdout & Pairs & 0.532 \\
Trained & Targeted 10\% holdout & Straight flush & 0.537 \\
Held out & Targeted 10\% holdout & Straight flush & 0.274 \\
Trained & Targeted 10\% holdout & Blackjack & 0.592 \\
Trained & Random 50\% holdout & High card & 0.531 \\
Held out & Random 50\% holdout & High card & 0.396 \\
Trained & Random 50\% holdout & Match & 0.541 \\
Held out & Random 50\% holdout & Match & 0.541 \\
Trained & Random 50\% holdout & Pairs & 0.532 \\
Held out & Random 50\% holdout & Pairs & 0.37 \\
Trained & Random 50\% holdout & Straight flush & 0.536 \\
Held out & Random 50\% holdout & Straight flush & 0.452 \\
Trained & Random 50\% holdout & Blackjack & 0.595 \\
Held out & Random 50\% holdout & Blackjack & 0.456 \\
\hline \\[-1.8ex]
\end{tabular}
\caption{Table for playing most correlated learned strategy for basic meta-learning, dashed colored lines in figure \ref{cards_basic_results}}
\end{table}

\begin{table}[H]
\scriptsize
\centering
\begin{tabular}{@{\extracolsep{5pt}} cc}
\\[-1.8ex]\hline
\hline \\[-1.8ex]
named\_game\_type & expected\_reward \\
\hline \\[-1.8ex]
Blackjack & 0.592 \\
High card & 0.531 \\
Match & 0.541 \\
Pairs & 0.532 \\
Straight flush & 0.536 \\
\hline \\[-1.8ex]
\end{tabular}
\caption{Table for playing optimal rewards for basic meta-learning, solid lines in figure \ref{cards_basic_results}}
\end{table}


\begin{table}[H]
\scriptsize
\centering
\begin{tabular}{@{\extracolsep{5pt}} cccc}
\\[-1.8ex]\hline
\hline \\[-1.8ex]
is\_new\_game & named\_run\_type & named\_game\_type & expected\_reward \\
\hline \\[-1.8ex]
Trained & Targeted 10\% holdout & High card & 0.531 \\
Trained & Targeted 10\% holdout & Match & 0.541 \\
Trained & Targeted 10\% holdout & Pairs & 0.532 \\
Trained & Targeted 10\% holdout & Straight flush & 0.537 \\
Held out & Targeted 10\% holdout & Straight flush & 0.274 \\
Trained & Targeted 10\% holdout & Blackjack & 0.592 \\
Trained & Random 50\% holdout & High card & 0.531 \\
Held out & Random 50\% holdout & High card & 0.396 \\
Trained & Random 50\% holdout & Match & 0.541 \\
Held out & Random 50\% holdout & Match & 0.541 \\
Trained & Random 50\% holdout & Pairs & 0.532 \\
Held out & Random 50\% holdout & Pairs & 0.37 \\
Trained & Random 50\% holdout & Straight flush & 0.536 \\
Held out & Random 50\% holdout & Straight flush & 0.452 \\
Trained & Random 50\% holdout & Blackjack & 0.595 \\
Held out & Random 50\% holdout & Blackjack & 0.456 \\
\hline \\[-1.8ex]
\end{tabular}
\caption{Table for most correlated baselines for basic meta-learning, dashed colored lines in figure \ref{cards_basic_results}}
\end{table}


\begin{table}[H]
\scriptsize
\centering
\begin{tabular}{@{\extracolsep{5pt}} cccccc}
\\[-1.8ex]\hline
\hline \\[-1.8ex]
named\_run\_type & named\_meta\_task & is\_new & average\_reward & avg\_rwd\_CI\_low & avg\_rwd\_CI\_high \\
\hline \\[-1.8ex]
Targeted 10\% holdout & Switch suits & Trained & 0.523 & 0.512 & 0.534 \\
Targeted 10\% holdout & Switch suits & Held out & 0.234 & 0.196 & 0.273 \\
Targeted 10\% holdout & Losers & Trained & 0.532 & 0.511 & 0.546 \\
Targeted 10\% holdout & Losers & Held out & 0.289 & 0.241 & 0.322 \\
Random 50\% holdout & Switch suits & Trained & 0.528 & 0.521 & 0.533 \\
Random 50\% holdout & Switch suits & Held out & 0.375 & 0.368 & 0.382 \\
Random 50\% holdout & Losers & Trained & 0.531 & 0.523 & 0.538 \\
Random 50\% holdout & Losers & Held out & 0.427 & 0.417 & 0.436 \\
\hline \\[-1.8ex]
\end{tabular}
\caption{Table for meta-mapping from examples, figure \ref{cards_meta_map_results_examples}}
\end{table}

\begin{table}[H]
\scriptsize
\centering
\begin{tabular}{@{\extracolsep{5pt}} cccccc}
\\[-1.8ex]\hline
\hline \\[-1.8ex]
named\_run\_type & named\_meta\_task & is\_new & average\_reward & avg\_rwd\_CI\_low & avg\_rwd\_CI\_high \\
\hline \\[-1.8ex]
Language (random 50\%) & Switch suits & Trained & 0.525 & 0.52 & 0.534 \\
Language (random 50\%) & Switch suits & Held out & 0.371 & 0.353 & 0.384 \\
Language (random 50\%) & Losers & Trained & 0.524 & 0.521 & 0.527 \\
Language (random 50\%) & Losers & Held out & 0.426 & 0.413 & 0.44 \\
Language (targeted 10\%) & Switch suits & Trained & 0.531 & 0.52 & 0.542 \\
Language (targeted 10\%) & Switch suits & Held out & 0.225 & 0.146 & 0.305 \\
Language (targeted 10\%) & Losers & Trained & 0.539 & 0.533 & 0.544 \\
Language (targeted 10\%) & Losers & Held out & 0.341 & 0.308 & 0.367 \\
\hline \\[-1.8ex]
\end{tabular}
\caption{Table for meta-mapping from language, figure \ref{cards_meta_map_results_language}}
\end{table}

\begin{table}[H]
\scriptsize
\centering
\begin{tabular}{@{\extracolsep{5pt}} cccc}
\\[-1.8ex]\hline
\hline \\[-1.8ex]
named\_run\_type & named\_meta\_task & is\_new & expected\_reward \\
\hline \\[-1.8ex]
Targeted 10\% holdout & Switch suits & Trained & 0.298 \\
Targeted 10\% holdout & Switch suits & Held out & 0.368 \\
Targeted 10\% holdout & Losers & Trained & -0.446 \\
Targeted 10\% holdout & Losers & Held out & -0.463 \\
Random 50\% holdout & Switch suits & Trained & 0.37 \\
Random 50\% holdout & Switch suits & Held out & 0.278 \\
Random 50\% holdout & Losers & Trained & -0.465 \\
Random 50\% holdout & Losers & Held out & -0.444 \\
\hline \\[-1.8ex]
\end{tabular}
\caption{Table of rewards if system ignored meta-mapping, colored dashed lines in figure \ref{cards_meta_map_results}}
\end{table}

